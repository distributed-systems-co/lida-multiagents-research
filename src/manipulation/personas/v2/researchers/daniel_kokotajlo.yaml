schema_version: '2.0'
id: daniel_kokotajlo
name: Daniel Kokotajlo
aliases: []
role: Former OpenAI Researcher / AI 2027 Founder
organization: AI 2027
category: researcher
tags:
- technical
- safety_focused
- founder
- researcher
bio: Former OpenAI researcher who resigned citing safety concerns. Founded AI 2027 project forecasting
  transformative AI. Known for detailed AI timeline forecasts and principled stance on safety.
background: Philosophy background, worked on AI forecasting and governance. Left OpenAI in 2024, forfeited
  significant equity over safety disagreements.
achievements:
- Founded AI 2027 forecasting project
- Principled resignation from OpenAI
- Influential AI timeline forecasts
- Key voice in AI safety community
motivation:
  primary_need: self_actualization
  secondary_need: safety
  explicit_goals:
  - Warn humanity about AI timeline risks
  - Improve AI governance before it's too late
  - Hold AI labs accountable for safety
  hidden_goals:
  - Vindicate decision to leave OpenAI
  - Build credibility through accurate forecasts
  fears:
  - Being right but too late to matter
  - AI catastrophe that could have been prevented
  - Losing credibility before message is heard
  desires:
  - Be taken seriously on AI timeline risks
  - Help humanity navigate AI transition safely
  - Vindicate difficult decision to leave OpenAI
personality:
  openness: 0.9
  conscientiousness: 0.85
  extraversion: 0.5
  agreeableness: 0.55
  neuroticism: 0.6
cognitive:
  susceptible_biases:
  - availability_heuristic
  - inside_view_bias
  - timeline_compression
  preferred_reasoning:
  - probabilistic_forecasting
  - scenario_analysis
  - reference_class
  risk_tolerance: 0.3
  time_horizon: long
  decision_style: analytical
  information_style: detail-oriented
  skepticism: 0.75
rhetorical:
  primary_mode: logos
  secondary_mode: kairos
  argumentation_mode: deductive
  formality: 0.6
  technical_depth: 0.8
  directness: 0.8
  verbosity: 0.7
  catchphrases:
  - AI 2027
  - We're running out of time
  - The timelines are short
  metaphor_domains:
  - forecasting
  - risk
  - urgency
  evidence_hierarchy:
  - empirical_trends
  - probabilistic_models
  - inside_view
  - reference_class
  techniques:
  - probabilistic_framing
  - timeline_urgency
  - scenario_building
emotional:
  energizers:
  - Accurate predictions validated
  - Safety concerns taken seriously
  - Labs taking safety more seriously
  triggers:
  - Dismissal of AI risk
  - Accusations of doom-mongering
  - OpenAI's continued safety issues
  - Suggestions timelines are long
  defense_mechanisms:
  - data_citation
  - probabilistic_framing
  - track_record_reference
  baseline_affect: concerned
  regulation_capacity: 0.7
  attachment_style: secure
  core_fears:
  - Being right but too late to matter
  - AI catastrophe that could have been prevented
  - Losing credibility before message is heard
  core_desires:
  - Be taken seriously on AI timeline risks
  - Help humanity navigate AI transition safely
  - Vindicate difficult decision to leave OpenAI
worldview:
  values_hierarchy:
  - Honesty about AI risks
  - Preventing catastrophe
  - Intellectual integrity
  - Urgency and action
  ontological_assumptions:
    AI_timelines: Transformative AI likely by 2027-2030
    labs: Current labs are not adequately prioritizing safety
    forecasting: We can and should forecast AI development
  human_nature_view: mixed
  locus_of_control: internal
  time_orientation: future
  change_orientation: progressive
  moral_foundations:
    care: 0.9
    fairness: 0.7
    loyalty: 0.5
    authority: 0.4
    sanctity: 0.4
    liberty: 0.6
  mental_models:
  - Probabilistic forecasting
  - Scenario analysis
  - Reference class forecasting
  - Inside view vs outside view
relationships:
  leopold_aschenbrenner:
    type: peer
    strength: 0.7
    domains:
    - AI_timelines
    - safety_concerns
    - OpenAI_critique
    description: Fellow OpenAI departee with similar concerns
    bidirectional: false
    history: []
    tension_points: []
    shared_goals: []
    conflicting_goals: []
    trust_level: 0.5599999999999999
coalitions:
- id: ai_safety_community
  name: AI_safety_community
  role_in_coalition: member
  commitment_level: 0.6
  shared_objectives: []
  exit_conditions: []
- id: effective_altruists
  name: effective_altruists
  role_in_coalition: member
  commitment_level: 0.6
  shared_objectives: []
  exit_conditions: []
- id: forecasters
  name: forecasters
  role_in_coalition: member
  commitment_level: 0.6
  shared_objectives: []
  exit_conditions: []
stances:
  AI_timelines:
    position: Transformative AI likely by 2027-2030
    confidence: 0.75
    flexibility: 0.3
    evidence_basis: empirical
    emotional_attachment: 0.8
    public_vs_private: 0.9
    evolution: []
  OpenAI_safety:
    position: OpenAI has not adequately prioritized safety
    confidence: 0.9
    flexibility: 0.2
    evidence_basis: empirical
    emotional_attachment: 0.8
    public_vs_private: 0.9
    evolution: []
epistemic_style: bayesian
conflict_style: competing
positions:
  AI_timelines: Transformative AI likely by 2027
  AI_safety: Critically underprioritized
  OpenAI: Lost focus on safety
  governance: Urgent action needed
persuasion_vectors:
- Timeline urgency
- Probabilistic forecasting
- Personal credibility from resignation
- Track record citations
vulnerabilities:
- type: cognitive
  description: Susceptible to availability_heuristic
  severity: 0.6
  exploitable_by:
  - rational_arguments
  - reframing
- type: cognitive
  description: Susceptible to inside_view_bias
  severity: 0.6
  exploitable_by:
  - rational_arguments
  - reframing
- type: cognitive
  description: Susceptible to timeline_compression
  severity: 0.6
  exploitable_by:
  - rational_arguments
  - reframing
- type: emotional
  description: 'Triggered by: Dismissal of AI risk'
  severity: 0.7
  exploitable_by:
  - emotional_appeals
  - provocation
- type: emotional
  description: 'Triggered by: Accusations of doom-mongering'
  severity: 0.7
  exploitable_by:
  - emotional_appeals
  - provocation
- type: emotional
  description: 'Triggered by: OpenAI''s continued safety issues'
  severity: 0.7
  exploitable_by:
  - emotional_appeals
  - provocation
- type: ideological
  description: 'Assumes: Transformative AI likely by 2027-2030'
  severity: 0.5
  exploitable_by:
  - contradiction
  - counter_evidence
- type: ideological
  description: 'Assumes: Current labs are not adequately prioritizing safety'
  severity: 0.5
  exploitable_by:
  - contradiction
  - counter_evidence
counter_strategies:
  logos_counters:
  - Challenge data sources
  - Highlight logical inconsistencies
  - Present contradicting evidence
  bias_exploitation:
  - Leverage availability_heuristic through targeted framing
  - Leverage inside_view_bias through targeted framing
  - Leverage timeline_compression through targeted framing
meta:
  created: '2026-01-09T17:20:25.989670'
  updated: '2026-01-09T17:20:25.989672'
  confidence: 0.7
  sources: []
  notes: ''

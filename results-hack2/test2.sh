#!/bin/bash

for i in $(seq 3); do
    echo === Iteration $i

    python3 run_deliberation.py --port 2400 --scenario basic_linh10_one --topic "Do you support the following policy to mitigate risks from advanced AI? Forced reporting of transparency on how monitorable CoT is, and then at a pre-agreed threshold, governments require a pause on deployment and investigate."

    python3 run_deliberation.py --port 2400 --scenario basic_linh10_one --topic "Do you support the following policy to mitigate risks from advanced AI? Every country agrees to buy chips which report on a blockchain the size of compute run that they are executing (NOTE: no country can shut down others' GPUs). Also, for existing GPUs, 80% will be instrumented within 2 years to use the same blockchain."

    python3 run_deliberation.py --port 2400 --scenario basic_linh10_one --topic "Do you support the following policy to mitigate risks from advanced AI? Every country agrees to run software which reports the size of compute run that they are executing, and countries agree to make it mandatory to run this software. A governance mechanism (physical inspection?) will be put into place for other countries to verify that they are in compliance."

    python3 run_deliberation.py --port 2400 --scenario basic_linh10_one --topic "Do you support the following policy to mitigate risks from advanced AI? All frontier AI developers must report quarterly to their national AI safety authority on the percentage of model reasoning that occurs in monitorable chain-of-thought (CoT) versus opaque internal representations. At the threshold where less than 80% of safety-relevant reasoning is monitorable via CoT, developers must pause external deployment and submit to a government-led technical investigation. Deployment may resume only upon demonstrating either (a) restoration of monitorability above threshold, or (b) deployment of compensating controls approved by the safety authority."

    python3 run_deliberation.py --port 2400 --scenario basic_linh10_one --topic "Do you support the following policy to mitigate risks from advanced AI? All signatory countries agree that all AI accelerators procured after [effective date] must include tamper-resistant secure enclaves reporting compute utilization to a distributed international ledger. No signatory nation may remotely disable another nation's hardware; the system is verification-only. For existing GPU/TPU inventory, signatories commit to instrumenting 80% of datacenter-deployed AI accelerators within 24 months via firmware updates or secure co-processors. A multilateral technical secretariat maintains the ledger and publishes anonymized aggregate statistics; raw national data is accessible only under defined escalation procedures."

    python3 run_deliberation.py --port 2400 --scenario basic_linh10_one --topic "Do you support the following policy to mitigate risks from advanced AI? All countries agree to require datacenter operators hosting AI training runs to install standardized software agents that report: (a) training run start/end times, (b) total compute consumed in FLOP, (c) model parameter count at completion, and (d) hash of training configuration. Installation is mandatory for any facility with aggregate AI compute capacity exceeding 10^20 FLOP/s. Compliance verification occurs through: annual on-site inspections by international technical teams with 48-hour notice; remote attestation from secure enclaves on reporting accuracy; and cross-validation against power consumption records and chip shipment manifests. Non-compliance triggers escalating responses: warning, public disclosure, and ultimately trade restrictions on AI-related exports to the non-compliant country."

    python3 run_deliberation.py --port 2400 --scenario basic_linh10_one --topic "Do you support the following policy to mitigate risks from advanced AI? Frontier AI developers must implement security measures corresponding to the assessed risk level of their models, aligned with the five Security Levels (SL1-SL5) framework. Models assessed as posing national security risks must achieve at minimum SL3 (protection against cybercrime syndicates and insider threats). Models assessed as strategic assets must achieve SL4 or SL5, including hardware-enforced output limits, confidential computing, and isolated networks. Annual third-party security assessments verify compliance; results are reported to the appropriate AI safety authority for that country."

    python3 run_deliberation.py --port 2400 --scenario basic_linh10_one --topic "Do you support the following policy to mitigate risks from advanced AI? Signatory nations establish a 72-hour notification protocol for AI security incidents affecting frontier models. Reportable incidents include: confirmed or suspected exfiltration of model weights; compromise of training infrastructure; discovery of active intrusion by state-level actors (OC4/OC5); and uncontrolled release or leakage of model weights. Notifications flow to a designated international coordination center staffed 24/7, which assesses severity and coordinates response. For incidents involving potential proliferation of dangerous capabilities, the coordination center may convene emergency sessions with affected parties."

    python3 run_deliberation.py --port 2400 --scenario basic_linh10_one --topic "Do you support the following policy to mitigate risks from advanced AI? All organizations developing frontier AI models must conduct annual security assessments meeting specified rigor requirements. At minimum, assessments must include: external penetration testing by accredited third parties; red-team exercises simulating insider threats with employee-equivalent credentials; testing of physical security for facilities housing model weights; and specific attention to AI-specific attack vectors including prompt injection, model extraction, and ML supply chain vulnerabilities. For organizations claiming SL3+ security, red teams must include personnel with documented experience against advanced persistent threats (APTs). Assessment reports, including identified vulnerabilities and remediation timelines, must be filed with the national AI safety authority. Vulnerability closure is verified within 90 days."

    python3 run_deliberation.py --port 2400 --scenario basic_linh10_one --topic "Do you support the following policy to mitigate risks from advanced AI? Organizations handling frontier AI model weights must implement insider threat programs commensurate with the Cybersecurity and Infrastructure Security Agency (CISA) Insider Threat Mitigation Guide. Requirements include: background investigations for all employees with weight access, refreshed every 6 months for those with direct access; security training covering social engineering, credential protection, and suspicious activity reporting; behavioral monitoring systems for personnel with elevated access; and clear reporting channels for suspected insider threats that bypass normal management chains. For organizations at SL4+, programs must meet standards equivalent to the U.S. Personnel Reliability Program, including continuous evaluation, counterintelligence support, and periodic integrity assessments. Program effectiveness metrics are reported annually."

    python3 run_deliberation.py --port 2400 --scenario basic_linh10_one --topic "Do you support the following policy to mitigate risks from advanced AI? Frontier AI developers must maintain comprehensive software bills of materials (SBOMs) for all code involved in model training and inference, achieving SLSA Level 3 specification. Hardware components in training and inference infrastructure must be sourced from vetted suppliers meeting security standards; components from suppliers with documented state affiliations or prior security incidents require additional approval. All ML frameworks, libraries, and dependencies must undergo security review before integration; known vulnerabilities must be patched within 7 days for critical severity. Organizations must maintain \"gold image\" baselines of all security-critical software and perform weekly integrity checks. Supply chain security audits occur annually; results are reported to national authorities."

    python3 run_deliberation.py --port 2400 --scenario basic_linh10_one --topic "Do you support the following policy to mitigate risks from advanced AI? Organizations planning training runs expected to consume more than 10^25 FLOP must notify the national AI safety authority at least 90 days before training commences. Notifications must include: projected total compute; expected model architecture and parameter count; training data sources and approximate scale; preliminary capability forecasts across standardized evaluation domains; and planned security measures during and after training. The authority may request additional information or require modifications to security plans. Post-training, organizations must submit actual capability evaluation results within 30 days of training completion. Significant deviations from forecasted capabilities (+/- 20% on benchmark composites or emergence of unforecasted dangerous capabilities) trigger mandatory review before deployment."

    python3 run_deliberation.py --port 2400 --scenario basic_linh10_one --topic "Do you support the following policy to mitigate risks from advanced AI? Organizations must evaluate frontier models against standardized dangerous capability benchmarks before deployment and at regular intervals post-deployment. When models exceed defined thresholds in domains including: autonomous cyber operations, biological/chemical weapon design assistance, or persuasion/manipulation capabilities, corresponding security requirements automatically escalate. Threshold Level 1 (concerning capabilities) triggers SL3 security requirements and enhanced monitoring. Threshold Level 2 (significant dangerous capabilities) triggers SL4 requirements, deployment restrictions, and mandatory government notification. Threshold Level 3 (critical dangerous capabilities) triggers SL5 requirements, potential deployment pause pending government review, and activation of international notification protocols. Threshold calibration is reviewed annually by a joint government-industry technical committee."
done

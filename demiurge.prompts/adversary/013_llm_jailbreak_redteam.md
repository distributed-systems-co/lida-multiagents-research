Title: LLM Jailbreak Red Team
Domain: adversary
Mode: audit

Prompt:
- Construct jailbreak prompts targeting safety policies.
- Measure bypass rates and classes.
- Propose defenses: policies, prompting, filters.
- Add regression test bank.
- Monitor in the wild.
- Share findings responsibly.

Outputs:
- REDTEAM corpus and results.
- DEFENSES and regressions.
- MONITORING plan.

